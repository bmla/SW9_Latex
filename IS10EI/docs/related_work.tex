% !TEX root = ../literature.tex
\section{Related work}
Probably should write something like an introduction and why we are looking at the sections bellow.
\subsection{(History of???)Research in Public Displays}
One of earliest large public display was Hole-in-Space, it was a mirror type, which enabled people from New York to see, hear and speak with head-to-toe, life sized images of people from the opposite side Los Angeles \cite{Galloway:1980}.
Thought the start was given as a artistic experiment, various research group later scientifically explored the concept of interactions between people located at a remove places over public displays with audio-video capabilities \cite{Fish:1990,Gaver:1992,Bly:1993}.

In the early nineties shared interactive display surfaces was an emerging research area, with earlies examples of of shared drawing surfaces \cite{Bly:1990, Ishii:1992}. Combining this with the new paradigm called " ubiquitous computing " envisioned by Weiser, he showed examples of how  display devices of different sized could be embedded into a working environment  to solve different tasks. He contemplated that these displays could be networked and also in the periphery of the user, leaving him with the choice of interaction. \cite{Weiser:1991} This two ideas resulted in  richer examples of situated displays \cite{Streitz:1999,Snowdon:2002}, which gave us a better understanding of the role that public displays have in conversation and also their influence on group dynamics.

In the late nineties we witnessed the emergence of ambient display systems. Systems that could project water ripples on ceiling of a room to denote different activities \cite{Ishii:1998}, or which used air bubbles rising in a vertically from a water tube to render small black and white images \cite{Heiner:1999}. Also wearable displays became increasingly popular as an area of research, leading to displays being designed and studied in a more compact form\cite{Borovoy:1999,Rhodes:1997,Falk:1999}. 

In the beginning of the new millennium, technological progress of display technologies gave us the possibilities to have more affordable and less cumbersome displays and projection techniques. With this research expanded to identify the social consequences of design and placement of displays. An example is Kaharalios et al work \cite{Karahalios:2004}, which even though involved connecting two remote locations over audio-video link, it had a different concept in mind. They blended the videos and converted them to make users look like graffiti, which was projected on the wall, this prompted users to move closer to the display and acted as a social catalyst to motivate users to engage in conversation. This led to a new focus of research encouraging social interactions using situated displays. Greenber et al.\cite{Greenberg:2001} combined multiple personal desktops and a
large semi-public display to improve awareness among colleagues  connected electronically. It consisted of a desktop client that allowed them to post multimedia content  on a real time collaborative surface. A collage with randomly placed
content appeared both on a large public display in a common area and on personal workstations. This acted as a start for social interaction between two people, for example, clicking on the live video stream would start a chat with that person.

Public displays used to support community and social activities revealed a major problem; the lack of willingness by users to participate.
\subsection{The Public Space as a challenge}
To address the participating problem, various researches presented different models on audience behavior and interaction with public displays.

 Streitz et al.\cite{Streitz:2003} presented the three-zone model which defined three zones of interaction; ambient zone, notification zone and cell interaction zone based on the distance between the user and the display. This model lacks support for multiple users and assumes that a user in the cell interaction zone intends to interact with the system. Brignull and Rogers \cite{Brignull:2003} presented an interaction model based on how people become aware of the existence of a display installation. They identified three activity spaces around a display installation: space of peripheral awareness, space of focal  awareness, and space of direct interaction. They further identified that the transition zones between these spaces represent a key barrier to interact with the display.

 Researchers became very interested in using mobile devices to interact with public  displays, when mobile devices became increasingly prevalent in the late 2000’s. The
 advantages put forward for mobile phones were three fold. Firstly, the sensors in mobile devices made it possible to detect people around a display. Secondly, physical or touch buttons, microphones, inertial
 sensors and cameras made it possible to make it an interaction device. Lastly, mobile
 phones acted as a storage medium to transfer information from and to public displays.
 
 \subsection{Interacting with Public Displays}
 One of the fundamental aspects of public displays that make them useful is their interactivity. This aspect encourages participation and enables users to explore the content. One way of doing that is using devices, such as mobile devices, which is known as cross-device interaction.
 
 Cross device studies with public displays are related to using mobile phones as interaction devices for controlling public displays, either with or without feedback on the mobile devices. An example of the former is the multiplayer Breakout game by Cheung et al. \cite{Cheung:2014} that is played on
 a large public display. Multiple players can join the game by scanning the QR code located in front of the display using their mobile phones. This web-based client application allowed each player to control their paddle by tilting their phone. The mobile device provides feedback regarding the game and connection status (error, connected, disconnected). Vepsalainen et al. \cite{Vepsalainen:2015} presented a similar work wherein mobile phones were used as a gamepad for a controlling the game, running on a large display, with feedback given on the mobile phones. Whereas, no feedback was provided on the users’
 mobile phones in Scroll, Tilt or Move, where Boring et al.\cite{Boring:2009} presented various techniques for controlling the pointer on a large display.
 
People use different modalities (speech, eye gaze, touch, gestures, facial expressions, body postures and others) while interacting with each other. Researchers have extended and explored the use of similar modalities to interact with public displays.

Speech can be recognized using microphone arrays near the displays to issue digital commands. For example, Ward provided air travel information based on the spontaneous speech commands given to the system \cite{Ward:1991}. However  using speech alone as an input method is error-prone especially when the content on the display is dynamic \cite{Oviatt:2000}, therefor majority of studies uses a combination of speech and gestures, as this combination was natural and efficient to cope up with the visual complexity of the display \cite{Angeli:1999}.

The term "gesture" is quite loosely defined in the field of HCI and it depends on the context of interaction. In our context, it usually refers to hand gestures and gestures using stylus-like devices or other external devices (such as the Wii or Kinect controller). The success of Kinect controller started a widespread interest in gesture interfaces within the research community. Users can walk into the vicinity and start interacting even before they
realize it, and explore the system gradually \cite{Muller:2012}. While gestural interaction is desirable for interaction with public displays, they pose some
challenges as well. A major challenge with gesture recognition is how to differentiate naturally occurring  gestures, and gestures intended to interact with the system \cite{Wexelblat:1997}. Moreover, the use of gestures could be embarrassing or disruptive to the user in a public environment \cite{Rico:2010}. 

Touch, is other natural modality we can use for interaction, is accurate and it provides a natural tactile feedback for the end of interaction. Although the rapid growth of touchscreen devices has increased the affordances for touch, not all displays currently deployed in public places support touch. Therefore, informing the user of which surfaces are touchscreens, and which are not, is crucial for its use. \emph{MirrorTouch} supported both mid-air gestures and touch to increase the usage of the display\cite{Muller:2014}. However, using touch is not always possible because of varying display locations and/or sizes.

As different interactions have different pros and cons an obvious question is would a combination of said interactions create a synergy that would maximize the pluses and minimizes the minuses for information transfer to public display?

 \subsection{Cross-device natural user interactions for data transfer in Public Displays}
 Determining the recipient and sender devices for an information transfer in a public  setting presents more challenges in contrast to controlled environments like offices, homes and workstations. One of the earlier attempts in this domain used a grid of QR codes to identify the desired
 content \cite{Ballagas:2005} the authors developed two complementary interaction techniques using phone cam: Sweep and Point+Shoot. In the sweep technique, the phone camera acts like an optical mouse. Successive images are compared to determine the direction and displacement of the phone. In Point+Shoot technique, the user aims the phone camera (point) at the desired content on the display and presses the joystick on their phone (shoot) to retrieve that content to their personal device via QR code. 
 
 A different idea is presented by Hardy and Rukzio \cite{Hardy:2008} allowed the user to touch the desired part of the public display with their mobile phone in order to perform interactions. This interaction technique was implemented using a grid of NFC tags, which would help the system dynamically identify the part of the application UI, the user was interacting with at that point. Since NFC tags have an id associated with them, it is trivial to identify the sender and recipient in the event of any information transfer between the public display and the personal device.  4
 
 Researchers have also explored the use of other sensing technologies to detect similar touch events. For example, PhoneTouch relied on accelerometer data generated whilst touching the display surface to generate touch events when the phone touches the display surface \cite{Schmidt:2012}. Users could share content with the display surface, simply by selecting the content on their phone, followed by touching the display surface with their phone. However, the major drawback of this approach becomes visible when the size of display increases beyond the physical reach of a person.
 
 To overcome this challenge Bragdon et al. propose using a system that combines touch + air gesture hybrid interactions. They aim to design, implement and test a system that allows a group of users to interact using air gestures and touch gestures. The purpose is to increase control, support democratic access, and share items across multiple personal devices such as smartphones and laptops where the {\em``primary design goal is fluid, democratic sharing of content on a common display.''} \cite{Bragton:2011}. This method enables access, control and sharing of information through several different devices such as multi-touch screen, mobile touch devices, and Microsoft Kinect sensors.
 
 Alt et al. \cite{Alt:2013} works,to allow also passersby (e.g., customers) to exchange content with public display, similar to traditional public notice areas, such as bulletin boards. They present Dignifes, a digital public notice area we built to investigate and compare possible interaction techniques. Based on a lab study that show that using direct touch at the display as well as using the mobile phone as a complementing interaction technology are most suitable. However their findings indicate that while users are similarly performant using mobile phones and displays. Users prefer mobile phones to create content in a privacy-preserving way / on-the-go.