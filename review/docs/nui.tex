% !TEX root = ../literature.tex
\subsection{Natural User interaction}
Natural user interactions are underlaying for the natural user interface. The "natural" in Natural User Interface is about how users feel and what they do when using a product. A product should take full advantage of the user's capabilities; the trick lies in making the user feel that it is natural from the start and not after a long time practicing. That does not mean that the product should be designed towards no learning requirements; it can be a better solution to make the user learn how to deal with the technology than making the technology deal with the complexity of humans. An example would be Apple's Newton Message Pad which was promised to understand human handwriting. The results were bad as the handwriting recognition was insufficiently robust. The successor Palm Pilot(1997) by Palm computing had more success, as the creators recognized the limitations of their technology. They developed an input language similar to roman characters that the users had to learn, however the experience felt natural because the recognizer worked so well.\cite{Wigdor:2011}\\ 


In 2001 D.F.Keefe et al. \cite{Keefe:2001} created a 3D virtual reality painting system where they used tangible items like a brush and cups for selecting brush type, to create an intuitive interface for artists who might not be familiar with virtual reality. The brush is tracked in the 3D space and the color palette is opened by making a circular gesture while holding the brush in an upward position; a color selection is made by moving the brush to a color and tilting the brush into another direction.
For advanced users a tracked glove can be used on the non-dominant hand to provide quick access to changing color and brush size.
The tracking however requires special cabled equipment, which results in additional weight on the hands and reduces the free movement. \\

Microsoft released the Kinect for Xbox 360 in 2010\cite{KinectFiction:2010} and the Kinect for Windows in 2012\cite{KinnectPower:2012}. The Kinect has a depth sensor, color camera sensor, and four-element microphone array. We found that the Kinect has been a tool for much research, many of them utilizing the depth sensor, which is the case of the following papers:\cite{Wilson:2010, Aigner:2012, Walter:2014}.\\

Wilson \cite{Wilson:2010} wanted to explore the application of depth-sensing cameras to detect touch on a tabletop. He found it to be less precise than touch screens, but with three advantages. The surface does not need to be instrumented; The surface is not required to be flat; and the information about the shape of the users and their arms can be exploited in useful ways, such as determining hover state or whether multiple touches is from the same user. \cite{Wilson:2010}\\


Motivated by surface and tabletop systems expansion to include proximity sensing, R.Aigner et al.\cite{Aigner:2012} looks at the Point-and-Click interface and User-Defined Interface(UDI). They find that Vogel and Balakrishnan \cite{Vogel:2005} built a Point-and-Click interface for large screen displays which simulates mouse input, and that several issues was discovered: precision in pointing, ambiguity of finger movements, and lack of physical feedback. They describe that it has been common to elicit gestures by applying the UDI methodology, but researchers have found that there is little consensus among users in association between gestures and their expected effect.\\

R.Aigner et al.\cite{Aigner:2012} believe that gesture languages have to be designed rather than observed, however a key limitation is the lack of guidelines for designing such languages.
They developed a Gesture type classification scheme based on the taxonomy of Karam and Schraefel \cite{Karam:2005} .
From their experiment they found that the participants used  a  wide  variety  of  gestures  to accomplish  the  same  effect, but the  type  of  gesture was  often  consistent  across  time and participant. Hence, their results suggest that \emph{"classifying by type reveals a much greater degree of consistency"} compared to eliciting gestures by using the UDI methodology. 
\\
\hlfix{
R.Walter et al.
%\protect\cite{Walter:2014}
 state that today's public displays only show predefined contents that passers-by are not able to change and argue that displays would benefit from techniques for interacting with large displays. They propose a design space for hand-gesture based mid-air selection techniques based on analysis of differences and similarities of existing mid-air selection techniques in NUI frameworks, commercial products and related work. From two lab studies they derived 4 selection techniques that supports immediate usability. In a field study they found that the 4 techniques performed equally and conclude that designers can choose freely between those 4 techniques.
They give their recommendation for designing of interactive public displays, that allows users to select options using mid-air gestures; and believe that their findings will make the platform of interactive public display more attractive to users.}{Move to Public space}
\\
%In this section we have explored literature on NUI, showing some of the early development and some of the later development where the Kinect has been used. From this we can deduce that there have been a focus on exploring the application of depth-sensing cameras and understanding and designing mid-air hand gestures. Their results have been recommendations and guidelines for selecting and designing hand gestures for interacting with the Kinect.

In this section we have explored literature on NUI, showing some of the early development and some of the later development where the Kinect has been used. From this we can deduce that there have been a focus on exploring the application of depth-sensing cameras as well as understanding and designing mid-air hand gestures.
Using the Kinect as a tool we can achieve interaction in a limited \hlfix{3Dspace}{Space or no space?} without the need for additional tools, such as tracked controllers or gloves. The Kinect can be used to achieve touch detection on non-instrumented surfaces or in midair by using the depth sensor to detect distance and recognize gestures.  R.Aigner et al.'\todo{missing 's' or ref?} and R.Walter et al.'\todo{missing 's' or ref?} results have been recommendations and guidelines for selecting and designing hand gestures for interacting with the Kinect. R.Aigner  et al. believes that a gesture language has to be designed instead of observed. Through the observation of participants communicating by using gestures in a lab study, they discover that the participants often used the same type of gesture to achieve a desired effect.
\hlfix{R.Walter et al. argues that public displays would benefit from techniques for interacting with large displays. They made a designspace which they tested to derive the selection techniques that supports immediate usability. In a field study they found that their 4 derived selection techniques performed equally and concludes that designers can choose freely among them.}{Move to public (space assumed)}
With these findings we are now aware of several applications where the Kinect can be used for interaction and which types of interaction techniques that provides the user with a good experience when using the Kinect.